## What is MFCS-Bench?  

MFCS-Bench is a benchmark system for evaluating the function-calling capabilities of large language models (LLMs) based on the **MFCS (Model Function Calling Standard)** protocol. It standardizes the evaluation methodology for how different models handle structured function calls, helping to build a more robust ecosystem of tool-using LLMs.  

The benchmark supports **multi-dimensional evaluation**, including metrics such as:  
- **Model accuracy**  
- **Token consumption**  
- **Response latency**  
- **Prompt adaptability**  

Users can simply modify the configuration file (e.g., model selection, tool list, and question sets), run the evaluation script, and generate a detailed reportâ€”enabling easy comparison across different configurations.  

For more features and usage instructions, visit the project homepage: [MFCS-Bench](https://github.com/mfcsorg/mfcs-bench).  